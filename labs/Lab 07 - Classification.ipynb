{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "Lab 07 - Classification.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAML_lBVgtK7",
        "colab_type": "text"
      },
      "source": [
        "# Lab 07\n",
        "\n",
        "This week, we'll learn a number of smaller skills:\n",
        "\n",
        "1. Getting Extracted Feature Files\n",
        "2. The Scipy Stack\n",
        "3. Pandas: Combining DataFrames\n",
        "4. Classification with Scikit Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHSU34iRgtK_",
        "colab_type": "text"
      },
      "source": [
        "## Getting Extracted Features Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cisf6IHgtLA",
        "colab_type": "text"
      },
      "source": [
        "*Update 2020: At small scales, you don't need to download Extracted Features files yourself, because the HTRC Feature Reader library in Python can do it automatically for you. e.g.*\n",
        "```\n",
        "from htrc_features import Volume\n",
        "vol = Volume('mdp.49015002392919')\n",
        "```\n",
        "In Colab, remember that you have to install the Feature Reader, which can be done with:\n",
        "\n",
        "```!pip install git+https://github.com/massivetexts/htrc-feature-reader.git```\n",
        "\n",
        "---\n",
        "\n",
        "There are 17 million books in the Hathitrust Extracted Features Dataset.\n",
        "\n",
        "To download the EF dataset file for a book, you need its HathiTrust ID. You can see this in the URL when you find books in the HathiTrust; e.g. for the Tom Sawyer book at https://babel.hathitrust.org/cgi/pt?id=nyp.33433042068894, the id is nyp.33433042068894.\n",
        "\n",
        "With the ID you can download the file at the following URL:\n",
        "\n",
        "   > `https://bedrock.resnet.cms.waikato.ac.nz/vol-checker/VolumeCheck?download-id={{VOLUME ID}}`\n",
        "\n",
        "For example:\n",
        "\n",
        "   > https://bedrock.resnet.cms.waikato.ac.nz/vol-checker/VolumeCheck?download-id=nyp.33433042068894\n",
        "\n",
        "Two things that we aren't focusing on, but which you can explore if you want large numbers of files for your final projects:\n",
        "- There are many ways to programmatically choose _many_ books at once, rather than looking up the books in the online interface. The easiest is to download the bibliographic metadata (called [Hathifile](https://www.hathitrust.org/hathifiles). This is a CSV file of all the available books. \n",
        "- The main way to download files or lists of files is using a command line application called `rsync`, after converting the IDs to a file path. The reference for doing so is here: [Syncing a list of files](https://github.com/htrc/htrc-feature-reader/blob/master/examples/ID_to_Rsync_Link.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_vnDzFgtLB",
        "colab_type": "text"
      },
      "source": [
        "## The Scipy Stack\n",
        "\n",
        "You guys are becoming Pandas pros! Pandas is the foundation of much data science work by professionals today. As we continue, increasingly we'll be learning Pandas-specific code and conventions rather than all of Python.\n",
        "\n",
        "Pandas is part of what is called the SciPy Stack: a selection of scientific tools that all work together. Here are the other ones:\n",
        "\n",
        " - **Numpy**: A mathematical library, offering ways to represent multidimensional arrays.\n",
        " - **Scipy**: Foundational scientific code.\n",
        " - **Pandas**: A nicer way of structuring and working with data, through DataFrame and Series objects. You can think of Pandas as a more flexible version of Numpy's arrays, where you can name the columns and work with the data with more semantics.\n",
        " - **Matplotlib**: A visualization library. We've seen this!\n",
        " - **IPython**: This is the special interactive version of Python that you use in Jupyter! So you don't have to write a script, run it, edit it, run it again, and so on.\n",
        " \n",
        "There are many tools that run very well with the SciPy stack and round out our data science environment:\n",
        "\n",
        "- **Scikit Learn**: Where Scipy is foundation tools, Scikit Learn gives you many advanced scientific algorithms for data science. Great documentation too: next week's clustering reading is from their documentation.\n",
        "- **Jupyter**: The web browser notebook-style way of using IPython.\n",
        "- **Seaborn**: Higher-level visualization tools. Matplotlib is like buying IKEA furniture: some assembly required. Seaborn is like buying pre-assembled furniture: much easier! Plus, just importing Seaborn into your code makes your Matplotlib code look nicer!\n",
        "- **Statmodels**: Similar to Scipy, statsmodels offers additional statistics models and tools.\n",
        "\n",
        "There are a few benefits due to how standard these tools are. First, they tend to play nice together. If you move into advanced libraries for really large scale analysis, those libraries tend to have the SciPy stack in mind too. Finally, they were installed by default when you installed Anaconda in the first week, so you have them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHkuPIhngtLC",
        "colab_type": "text"
      },
      "source": [
        "## Pandas: Combining DataFrames\n",
        "\n",
        "To combine multiple DataFrames, you can use `pd.concat()` on a list of DataFrames (i.e. [dataframe1, dataframe2, etc.] ). For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlse1lRpgtLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "dacb81c7-2814-4303-eeed-8aaf52ca52b7"
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.DataFrame([(1,'a'), (2,'b')])\n",
        "test"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1\n",
              "0  1  a\n",
              "1  2  b"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEGY9vc1gtLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "747a985e-583a-489a-a404-9767d4723983"
      },
      "source": [
        "# combining two of the same dataframe:\n",
        "list_of_dataframes = [test, test]\n",
        "combined = pd.concat(list_of_dataframes)\n",
        "combined"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1\n",
              "0  1  a\n",
              "1  2  b\n",
              "0  1  a\n",
              "1  2  b"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb3t7WrkgtLM",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes Classification with Scikit Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbaoNNAQgtLN",
        "colab_type": "text"
      },
      "source": [
        "I've prepared a set of training documents and testing documents for a French/English classifier in [english_french_class.csv](https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class.csv).\n",
        "\n",
        "Load a CSV to a dataframe as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX2ty2GAgtLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "100e721c-b3b5-4bcd-a68d-6ac13c2fca51"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class.csv'\n",
        "data = pd.read_csv(url, encoding='utf-8').set_index('book')\n",
        "data.head(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>!</th>\n",
              "      <th>!—</th>\n",
              "      <th>!—the</th>\n",
              "      <th>\"</th>\n",
              "      <th>\"\"</th>\n",
              "      <th>\"because</th>\n",
              "      <th>\"if</th>\n",
              "      <th>\"it</th>\n",
              "      <th>\"only</th>\n",
              "      <th>\"or</th>\n",
              "      <th>\"what</th>\n",
              "      <th>\"why</th>\n",
              "      <th>\"you</th>\n",
              "      <th>\"you're</th>\n",
              "      <th>#</th>\n",
              "      <th>$</th>\n",
              "      <th>%</th>\n",
              "      <th>&amp;</th>\n",
              "      <th>'</th>\n",
              "      <th>'\"</th>\n",
              "      <th>''</th>\n",
              "      <th>'.</th>\n",
              "      <th>'a</th>\n",
              "      <th>'about</th>\n",
              "      <th>'ackley</th>\n",
              "      <th>'ai</th>\n",
              "      <th>'aime</th>\n",
              "      <th>'aimer</th>\n",
              "      <th>'aimez</th>\n",
              "      <th>'and</th>\n",
              "      <th>'appelle</th>\n",
              "      <th>'apprit</th>\n",
              "      <th>'arrête</th>\n",
              "      <th>'as</th>\n",
              "      <th>'at</th>\n",
              "      <th>'auroit</th>\n",
              "      <th>'avait</th>\n",
              "      <th>'avez</th>\n",
              "      <th>'aviez</th>\n",
              "      <th>'avoir</th>\n",
              "      <th>...</th>\n",
              "      <th>ﬁghter</th>\n",
              "      <th>ﬁghting</th>\n",
              "      <th>ﬁgure</th>\n",
              "      <th>ﬁlled</th>\n",
              "      <th>ﬁlthy</th>\n",
              "      <th>ﬁnally</th>\n",
              "      <th>ﬁnd</th>\n",
              "      <th>ﬁne</th>\n",
              "      <th>ﬁnger</th>\n",
              "      <th>ﬁngernails</th>\n",
              "      <th>ﬁngers</th>\n",
              "      <th>ﬁnish</th>\n",
              "      <th>ﬁnished</th>\n",
              "      <th>ﬁre</th>\n",
              "      <th>ﬁred</th>\n",
              "      <th>ﬁrm</th>\n",
              "      <th>ﬁrmly</th>\n",
              "      <th>ﬁrst</th>\n",
              "      <th>ﬁrst-aid</th>\n",
              "      <th>ﬁsh</th>\n",
              "      <th>ﬁshing</th>\n",
              "      <th>ﬁst</th>\n",
              "      <th>ﬁsts</th>\n",
              "      <th>ﬁve</th>\n",
              "      <th>ﬂak</th>\n",
              "      <th>ﬂat</th>\n",
              "      <th>ﬂed</th>\n",
              "      <th>ﬂesh</th>\n",
              "      <th>ﬂew</th>\n",
              "      <th>ﬂies</th>\n",
              "      <th>ﬂight</th>\n",
              "      <th>ﬂights</th>\n",
              "      <th>ﬂoor</th>\n",
              "      <th>ﬂown</th>\n",
              "      <th>ﬂuid</th>\n",
              "      <th>ﬂung</th>\n",
              "      <th>ﬂush</th>\n",
              "      <th>ﬂushed</th>\n",
              "      <th>ﬂy</th>\n",
              "      <th>ﬂying</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>book</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hvd.32044014292023</th>\n",
              "      <td>868.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4582.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hvd.32044102860673</th>\n",
              "      <td>1354.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 16115 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         !   !—  !—the       \"  ...  ﬂush  ﬂushed   ﬂy  ﬂying\n",
              "book                                            ...                          \n",
              "hvd.32044014292023   868.0  0.0    0.0  4582.0  ...   0.0     0.0  0.0    0.0\n",
              "hvd.32044102860673  1354.0  0.0    0.0   139.0  ...   0.0     0.0  0.0    0.0\n",
              "\n",
              "[2 rows x 16115 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3TvoUPVgtLR",
        "colab_type": "text"
      },
      "source": [
        "This loaded a 'wide' DataFrame, where each row is a book, each column is a word, and the cell $value_{row, column}$ is the count for that $word_{column}$ in $book_{row}$. The book ids were also a column, but we converted those to an index after loading with `set_index('book')`. I'll detail later how this information was collected.\n",
        "\n",
        "There is also a CSV with the [truth labels](https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class_labels.csv) for each book:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gp6xDohigtLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "538dc8f5-817d-44d9-c485-44470eaa515f"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/classification/english_french_class_labels.csv'\n",
        "labels = pd.read_csv(url, encoding='utf-8')\n",
        "labels.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>book</th>\n",
              "      <th>title</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hvd.32044014292023</td>\n",
              "      <td>Alice's adventures in Wonderland ; and, Throug...</td>\n",
              "      <td>eng</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hvd.32044102860673</td>\n",
              "      <td>Notre Dame de Paris. Abridged and edited, with...</td>\n",
              "      <td>fre</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 book  ... language\n",
              "0  hvd.32044014292023  ...      eng\n",
              "1  hvd.32044102860673  ...      fre\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4uCex-ZgtLV",
        "colab_type": "text"
      },
      "source": [
        "For train/test, we'll use half of the documents to build a classifier and the other half to test it.\n",
        "\n",
        "(`iloc` allows you to slice dataframes by number; e.g. `iloc[0:6]`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP3UJTYPgtLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = data.iloc[0:6]\n",
        "train_labels = labels.iloc[0:6]\n",
        "\n",
        "test_data = data.iloc[6:]\n",
        "test_labels = labels.iloc[6:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEKsUWa7gtLZ",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes classification is straightforward to use with Scikit Learn. To train, you need data and the correct classes:\n",
        "\n",
        "```python \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(training_data, training_labels)\n",
        "```\n",
        "\n",
        "To predict the class for unknown books, format their word frequencies in the same order and pass the information to the classifier:\n",
        "\n",
        "```python\n",
        "classifier.predict(new_data)\n",
        "```\n",
        "\n",
        "Lets try it for real:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cer73disgtLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4ed92c7-6e07-4361-e9cf-5cffd3541983"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "# Train our model!\n",
        "classifier.fit(train_data, train_labels['language'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkImHB3RgtLh",
        "colab_type": "text"
      },
      "source": [
        "And now load a new set of books and predict them. The books of the test dataset are:\n",
        "\n",
        "1. Les caves du Vatican\n",
        "2. Madame Bovary\n",
        "3. Jean Barois\n",
        "4. Catch-22\n",
        "5. The Catcher in the Rye\n",
        "6. The Lord of the Rings\n",
        "\n",
        "Let's predict their languages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "r_E7jeWEgtLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b1c7a4c-f3bb-49e2-b70b-b38f349f607d"
      },
      "source": [
        "# Predict the language of another book\n",
        "classifier.predict(test_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['fre', 'fre', 'fre', 'eng', 'eng', 'eng'], dtype='<U3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T03cXt0KgtLl",
        "colab_type": "text"
      },
      "source": [
        "Perfect classification!\n",
        "\n",
        "For most classification tasks, the accuracy is lower. Languages are very distinct, however. You can see the underlying information from the classifier with `classifier.predict_log_proba(test_data)`: zero shows the chosen class and the closer to zero the other values are, the closer their class probability was to the one that was eventually selected.\n",
        "\n",
        "These books are 'test' documents because we know the real answer. The true labels can be given to `classifier.score`, to count what proportion of classifications are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j59htGWigtLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69266787-af22-4055-e804-db4c036a3952"
      },
      "source": [
        "classifier.score(test_data.values, test_labels['language'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6jXV2RtgtLp",
        "colab_type": "text"
      },
      "source": [
        "There isn't much complexity to this code. The tricky parts are in getting the data structured properly. Scikit Learn doesn't keep the column names from Pandas, it just pulls out the values. So if your training data looks like:\n",
        "\n",
        "```document 1: [word X count, word Y count, ... word Z]\n",
        "document 2: [word X count, word Y count, ... word Z]```\n",
        "\n",
        "Then you need to make sure that your future documents order their counts as X, Y, ... Z.\n",
        "\n",
        "To better see the information Scikit Learn is using, consider this test DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moLYNUiPgtLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0f2c5233-d001-4b6c-a297-4b551261ef53"
      },
      "source": [
        "test_df = pd.DataFrame([[1,2,3], [4,5,6]], columns=['A', 'B', 'C'], index=['i', 'ii'])\n",
        "test_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ii</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    A  B  C\n",
              "i   1  2  3\n",
              "ii  4  5  6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVsAcwygtLx",
        "colab_type": "text"
      },
      "source": [
        "This is the information that the classifier actually uses:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZG2p9ehgtLy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a6b7bfad-02fb-421b-ba44-a4f0d7a437fc"
      },
      "source": [
        "test_df.values"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUBfguxAgtL1",
        "colab_type": "text"
      },
      "source": [
        "It looks like a list of lists, doesn't it? It's a Numpy array, which for now you can think as a smarter, faster version of a list of lists. What matters is remembering that it is just numbers arranged in two dimensions, so don't expect the classifier to know what word each number refers to.\n",
        "\n",
        "The truth labels are just one dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7r766hygtL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da28eb67-9ed1-4d8b-b103-d4ade245a006"
      },
      "source": [
        "test_labels['language'].values"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['fre', 'fre', 'fre', 'eng', 'eng', 'eng'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_FlavDGgtL5",
        "colab_type": "text"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccla__KEgtL5",
        "colab_type": "text"
      },
      "source": [
        "Load the following data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a49RCiq-gtL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/contemporary_books/'\n",
        "data = pd.read_csv(path + 'contemporary.csv', encoding='utf-8').set_index('book')\n",
        "labels = pd.read_csv(path + 'contemporary_labels.csv', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1eiYliZgtL9",
        "colab_type": "text"
      },
      "source": [
        "You'll be building a classifier for author."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hAMG347gtL_",
        "colab_type": "text"
      },
      "source": [
        "**Q1**: What are the 3 classes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMU_OgwBgtMC",
        "colab_type": "text"
      },
      "source": [
        "**Q2**: Show the code to split the data and truth labels into a test and train dataset, using the first fifteen books for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTzZ9_6PgtMD",
        "colab_type": "text"
      },
      "source": [
        "**Q3**: Create and train a Multinomial Naive Bayes classifier. Paste your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZDWyhX5gtME",
        "colab_type": "text"
      },
      "source": [
        "**Q4**: What are the classifier's predictions on the test code? Fill in the numbered values below:\n",
        "\n",
        "```\n",
        "array(['Grisham', '[[1]]', 'Atwood', 'King', 'Grisham', '[[2]]', 'King',\n",
        "       'Grisham', 'King', '[[3]]', 'Atwood', 'Atwood', 'Grisham', 'King',\n",
        "       '[[4]]', 'King'], \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C-26F6ogtMF",
        "colab_type": "text"
      },
      "source": [
        "**Q5**: What is the classifier's accuracy to four decimal places? i.e. X.XXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx6ETA8UgtMG",
        "colab_type": "text"
      },
      "source": [
        "**Q6**: Which books were classified correctly or incorrectly:\n",
        "\n",
        "- Cell by Stephen King: [Correct, Incorrect]\n",
        "- The Handmaid's Tale by Margaret Atwood: [Correct, Incorrect]\n",
        "- Danse macabre by Stephen King: [Correct, Incorrect]\n",
        "- Cat's eye by Margaret Atwood: [Correct, Incorrect]\n",
        "\n",
        "_Make sure you're getting the correct answer before continuing._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfGgMb2rgtMG",
        "colab_type": "text"
      },
      "source": [
        "**Q7**: Build a classification report with SciKit Learn as [shown here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#evaluation-of-the-performance-on-the-test-set). The 'names' of classes are in `classified.classes_`. Fill in the missing precision and recall values below:\n",
        "\n",
        "```\n",
        "             precision    recall    f1-score   support\n",
        "\n",
        "     Atwood       [[1]]     1.00       0.91         5\n",
        "    Grisham       1.00      [[2]]      1.00         5\n",
        "       King       1.00      0.83       0.91         6\n",
        "\n",
        "avg / total       0.95      [[3]]      0.94        16\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaYlq8O0gtMH",
        "colab_type": "text"
      },
      "source": [
        "**Q8**: Try to train a new classifier, without smoothing. In other words, set classifier.alpha to 0 before training. What happens to the predictions and what do the underlying probability patterns point to as the reason?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0wEC2degtMI",
        "colab_type": "text"
      },
      "source": [
        "**Q9**: [2 marks] [This printing of Sense and Sensibility](https://catalog.hathitrust.org/Record/008663968) has two volumes. I'm interested in looking at word patterns for the combined set of volumes all at once. How would you download the Extracted Features files for both volumes, and what code would you use to read and join the token count DataFrames into one long DataFrame?"
      ]
    }
  ]
}