{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "Lab 08 - Clustering.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/organisciak/Text-Mining-Course/blob/independentstudy/labs/Lab%2008%20-%20Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6vnfN6ETP0b",
        "colab_type": "text"
      },
      "source": [
        "# Lab 08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9zVSzYRTP0e",
        "colab_type": "text"
      },
      "source": [
        "## More Classification with Scikit Learn\n",
        "\n",
        "Last week we used the Naive Bayes multinomial classifier, getting 93% accuracy on our author classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeMUL7qITP06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "path = 'https://raw.githubusercontent.com/organisciak/Text-Mining-Course/master/data/contemporary_books/'\n",
        "data = pd.read_csv(path + 'contemporary.csv', encoding='utf-8').set_index('book')\n",
        "info = pd.read_csv(path + 'contemporary_labels.csv', encoding='utf-8')\n",
        "\n",
        "train_data = data.iloc[0:15]\n",
        "train_labels = info.iloc[0:15]\n",
        "test_data = data.iloc[15:]\n",
        "test_labels = info.iloc[15:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMz3M_LHTP47",
        "colab_type": "text"
      },
      "source": [
        "Other classifiers are available from SciKit Learn using the same interface, including support vector machines ([linear_model.SGDClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)) and Logistic Regression: ([linear_model.LogisticRegression()](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). They have many more options to customize them; for now, you can focus on the default options. Out of the box, you use them in exactly the same way that you used MultinomialNB last week.\n",
        "\n",
        "**Q1**: Replacing the Naive Bayes classifier with Logistic Regression, what accurance do you get on the same train/test task as in Lab 8? Don't forget to start with `from sklearn import linear_model`.\n",
        "\n",
        "[0.0, 0.25, 0.5, 0.625, 0.6875, 0.875, 0.9375, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abEKaSGaTP5l",
        "colab_type": "text"
      },
      "source": [
        "**Q2**: Try the SVM classifier. Re-run your code initializing, fitting, and scoring a couple of times. What happens to the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW2BjVoCTP5q",
        "colab_type": "text"
      },
      "source": [
        "## Clustering with Scikit Learn\n",
        "\n",
        "To cluster in Scikit Learn, you need the same type of data as in classification: a set of vectors with term counts, one vector per document.\n",
        "\n",
        "However, clustering is usually done when you don't know a real set of classes, so you do not supply labels.\n",
        "\n",
        "Here is how you might learn a set of clusters with K-Means from the Atwood/Grisham/King data that we already have loaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd1TgzI2TP5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BQcvPBjTP5-",
        "colab_type": "text"
      },
      "source": [
        "The cluster groups that were assigned are available from `kmeans.labels_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di7A6zEpTP6C",
        "colab_type": "code",
        "colab": {},
        "outputId": "ed539cac-4576-4b2b-bb2b-5835e2730b76"
      },
      "source": [
        "kmeans.labels_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0,\n",
              "       0, 0, 0, 0, 0, 2, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FkeMZaWTP6N",
        "colab_type": "text"
      },
      "source": [
        "Remember that K-Means starts with a random initialization and can get stuck in different configurations, so your groups may not be identical.\n",
        "\n",
        "**Q3**: You want to view the cluster groups alongside the information for the books that are held in info. Show the code to add a `group` column to `info`, set to the kmeans label information, and to sort the values by that column. It will look something like this: \n",
        "\n",
        "![](https://github.com/organisciak/Text-Mining-Course/blob/independentstudy/images/lab8-group-column.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEYntOBgTP6O",
        "colab_type": "text"
      },
      "source": [
        "In my K-Means clustering for 3 groups, I ended up with one group that only had one book: *The Stand*.\n",
        "\n",
        "This makes sense, because *The Stand* is loooong, and I'm using raw term frequencies. So, every word will show up a lot more, not because that word is more prominant in that book, but because there are some many more words being used overall. On a graph, *The Stand* would float far afield of all the other books.\n",
        "\n",
        "We already learned how to correct raw frequency counts: our old friend tf-idf! We learned about softened the scaling of counts (sublinear tf) and weighing words relative to their prevalence in the language (idf). Scikit Learn offers both of those under `sklearn.feature_extraction.text.TfidfTransformer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk08jANFTP7K",
        "colab_type": "text"
      },
      "source": [
        "**Q4**: Using the documentation for TfidfTransformer (and maybe help from your classmates), determine how to transform the document-term data used above, with both sublinear tf scaling and inverse-document-frequency. Share your code.\n",
        "\n",
        "Hint: if you did it right, the value for the first word of the first document for your new variable (i.e. `data_tfidf[0,0]`) should be around `0.0063`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1fSxbvPTP7N",
        "colab_type": "text"
      },
      "source": [
        "**Q5**: Try clustering on the tf-idf transformed data, using the same `n_clusters=3` k-means options as before. How do the results look different? (tip, the sorting from *Q3* might help you look at the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6FP_JWTP7R",
        "colab_type": "text"
      },
      "source": [
        "So far we have tested clustering around known information, but the useful part of unsupervised learning is in finding new patterns. What if you cluster into two clusters? 20? What patterns do we see?\n",
        "\n",
        "**Q6**: Cluster data_tfidf to 2 clusters and describe how the books cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N21AiGBWTP7g",
        "colab_type": "text"
      },
      "source": [
        "## More Techniques\n",
        "\n",
        "### An easier to interpret classifier: decision tree\n",
        "\n",
        "Here is an example of a decision tree classifier, which identifies discriminatory features out of the larger set, and uses those features to make the important classification decisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-qRf1q3TP7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(data, info['author'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mVMX_ToTP7v",
        "colab_type": "text"
      },
      "source": [
        "Visualization is trickier because you need to install a separate library; details can be found in the Scikit Learn docs. Here is an example of what a decision tree might look like:\n",
        "\n",
        "![](https://github.com/organisciak/Text-Mining-Course/blob/independentstudy/images/decisiontree.png?raw=1)\n",
        "\n",
        "(This tree was trained with the arguments `splitter='random', criterion='entropy', min_samples_split=0.5`.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASEGHQy_TP7x",
        "colab_type": "text"
      },
      "source": [
        "This example uses all of the data, because we're not testing it on anything, but it is possible to use in the same way as earlier classifiers, with `clf.fit(train_data)` and `clf.predict(test_data)`.\n",
        "\n",
        "Decision Trees are easy to interpret, but tend to be most interesting for tasks with many classes but few features. For text mining, we usually have really large feature sets, because each word in our vocabulary is a feature.\n",
        "\n",
        "Here is a non-text example that I trained on the survival of Titanic passengers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qVd5T__nTP8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/carData/TitanicSurvival.csv', na_values='*').dropna()\n",
        "# Make the passenger class column numeric (e.g. 1,2,3) instead of strings (e.g. '1st', '2nd', '3rd')\n",
        "df['passengerClass'] = df['passengerClass'].apply(lambda x: float(x[0]))\n",
        "df['sexCode'] = df['sex'] == 'Female' # True if matches, False if not Female\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(df[['passengerClass', 'age', 'sexCode']], df['survived'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYju88BVTP8R",
        "colab_type": "text"
      },
      "source": [
        "The decision tree for this classifier is [posted as a PDF](../images/titanic.pdf). Reading from the top:\n",
        "\n",
        "- By default, assume that the passenger didn't survive.\n",
        "- The most important indicator of survival is gender. If the gender is '0' (Male), assume the person died, else assume they survived.\n",
        "- Amongst men, Age <= 12 was an indicator of survival. Amongst women, the most important next differentior was whether they were in first class (survived) or third (did not survive).\n",
        "\n",
        "And so on. The zoomed out view shows the grimness: men on left, women on right, dark orange is likely non-survival while dark blue is likely survival.\n",
        "\n",
        "![](https://github.com/organisciak/Text-Mining-Course/blob/independentstudy/images/titanic-zoomed-out.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "188Qhyy2TP8S",
        "colab_type": "text"
      },
      "source": [
        "### Agglomerative Clustering\n",
        "\n",
        "Here is an example of agglomerative cluster - 'bottom-up' clustering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1PU0xFJTP8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Learn clusters\n",
        "model = AgglomerativeClustering()\n",
        "model = model.fit(data_tfidf.toarray())\n",
        "\n",
        "# Visualize the clusters\n",
        "Z = linkage(model.children_)\n",
        "graph = dendrogram(Z, orientation='right', labels=info['title'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fuun9Jg5TP8y",
        "colab_type": "text"
      },
      "source": [
        "These clusters are visualized using a dendrogram. The lines connect similar books or clusters, and the depth of the lines shows how similar those two nodes are. For example the cluster of _Cujo_ and _Dance macabre_ is very dissimilar from the cluster of all the other books, which is why the blue line extends so far to the right when connecting them."
      ]
    }
  ]
}